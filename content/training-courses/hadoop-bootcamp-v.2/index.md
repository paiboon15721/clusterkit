---
title: "Hadoop Bootcamp V.2"
subID: "5e32c6df369cdf2fdc2896f9"
date: 2019-12-19T13:47:20+07:00
draft: false
weight: 40
totalHours: 18
totalDays: 3
price: 14980
pdfURL: "http://clusterkit.co.th/training/pdf/Hadoop_Bootcamp_V2.pdf"
---

## รายละเอียดหลักสูตร

หลักสูตรนี้มุ่งหมายให้ผู้เรียนเข้าใจกระบวนการทำงานของระบบHadoopและสามารถติดตั้งระบบHadoop Clusterเพื่อใช้งานรวมถึงเข้าใจเครื่องมือแต่ละตัวและประยุกต์ใช้งานซอฟต์แวร์เหล่านั้นได้ ในเนื้อหาเป็นการลงมือปฏิบัติคอนฟิกเครื่องเซิร์ฟเวอร์คลัสเตอร์ให้ทำงานร่วมกัน และศึกษาส่วนประกอบหลัก ๆ ของHadoopไล่ไปทีละส่วน ตั้งแต่ส่วนของระบบไฟล์แบบกระจายที่เรียกว่าHadoop Distributed File System (HDFS) การประมวลผลข้อมูลด้วย MapReduce รวมถึงซอฟต์แวร์แวดล้อมที่มาทำงานบนระบบ MapReduce อย่าง Pig และHive เพื่อใช้จัดการกับข้อมูลในรูปภาษาสคริปต์ และภาษาในลักษณะ SQLตามลำดับ นอกจากนั้นยังได้หัดใช้ Sqoop เพื่อเชื่อมต่อกับซอฟต์แวร์ฐานข้อมูล (DBMS) รวมถึงการติดตั้งและใช้งาน Hue, impala และ spark ผู้เรียนจะได้ศึกษาไปทีละขึ้น รวมถึงจะได้เรียนรู้คำสั่งจำเป็นต่อการดูแลระบบ การอ่านและวิเคราะห์ Log File

## หลักสูตรนี้เหมาะสำหรับ

ผู้ที่สนใจเรื่อง Big Data และ Hadoop วิศวกรคอมพิวเตอร์ นักเทคโนโลยีสารสนเทศ อาจารย์และบุคลากรทางการศึกษาในสาขาที่เกี่ยวข้อง

## วัตถุประสงค์

1. เพื่อให้ผู้เข้าอบรมมีความรู้ความเข้าใจเกี่ยวกับ Big Data
2. เพื่อให้ผู้เข้าอบรมเข้าใจในหลักการทำงานของซอฟต์แวร์ Hadoop
3. เพื่อให้ผู้เข้าอบรมสามารถติดตั้งระบบ Hadoop Cluster ขึ้นใช้งานเองได้
4. เพื่อให้ผู้เข้าอบรมรู้จักกับเครื่องมือแวดล้อมต่าง ๆ บน Hadoop เช่น Hive, Spark และสามารถนำไปประยุกต์ใช้งานได้
5. เพื่อให้ผู้เข้าอบรมมีทักษะในการวิเคราะห์ปัญหาที่เกิดขึ้นและแก้ไขได้

## ความรู้พื้นฐาน

ผู้เข้าอบรมควรมีความสามารถในการใช้งานคำสั่งลีนุกซ์ (Linux) พื้นฐาน และควรมีความเข้าใจเรื่องระบบเครือข่ายและไฟล์วอล์

## ซอฟต์แวร์ที่ใช้

1. Cloudera Hadoop (CDH6) or Hortonworks Data Platform (HDP3)
2. JDK-1.8
3. CentOS-7 x86_64
4. VirtualBox (ทีมงานคลัสเตอร์ตคิทจะเตรียม VirtualBox Image ที่ติดตั้ง Linux CentOS-7 ไว้ให้)
5. OpenLandscape Cloud ( ผู้เรียนจะได้ใช้งานคลาวด์จำนวน 6 VMs มีหน่วยความจำขนาด 1x16GB และ 5x4GB ตลอด 3 วัน)

## สิ่งที่ผู้เข้าอบรมต้องเตรียม

ผู้เข้าอบรมต้องเตรียมเครื่องคอมพิวเตอร์ที่มีหน่วยความจำไม่น้อยกว่า 4 GB และมีพื้นที่ว่าง (Disk space) ไม่น้อยกว่า 50GB มาในการอบรม พร้อมติดตั้ง VirtualBox และ VirtualBox Extension Pack และเปิดฟังก์ชั่น Virtualization ใน BIOS มาให้เรียบร้อยตาม{{< link url="http://www.clusterkit.co.th/training/pdf/VirtualBox_64bit_Problem.pdf" text="คู่มือ" >}}

## เนื้อหาหลักสูตร

### วันที่ 1

- แนะนำ Big Data ในภาพรวม
- เข้าใจการทำงานและรู้จักองค์ประกอบของ Hadoop
- แนะนำ Cloudera Hadoop และ Hortonworks Data Platform
- การติดตั้ง JDK
- การปรับแต่งระบบลีนุกซ์เพื่อเตรียมติดตั้ง Hadoop แบบคลัสเตอร์
  - การสร้าง ssh key และวางคีย์เพื่อสร้างสภาพแวดล้อมแบบ Single Sign On
  - การปรับแต่งไฟล์วอลล์เพื่อความปลอดภัย
  - การกำหนดค่าไฟล์ /etc/hosts
  - การปิด selinux
- ติดตั้งและใช้งาน HDFS (ติดตั้งบนสภาพแวดล้อม 3 เครื่อง ประกอบด้วย 1 Name Node และ 2 Data Node)
  - การออกแบบระบบ HDFS
  - รู้จักกับค่าคอนฟิกกูเรชั่นที่เกี่ยวข้อง
  - การตรวจดูสถานะและใช้งานหน้าเว็บ HDFS
  - การใช้คำสั่ง hadoop การจัดการไฟล์ในระบบ HDFS
  - การตรวจดูสถานะ HDFS ผ่านคำสั่งที่เกี่ยวข้อง เช่น dfsadmin
  - การอ่านLog File และการวิเคราะห์ปัญหาที่เกิดขึ้น
  - การจัดการบัญชีผู้ใช้งาน
- ติดตั้ง Hadoop ผ่าน Cloudera Manager หรือ Apache Ambari (Hortonwork) ผู้เรียนสามารถเลือกติดตั้งได้ โดยติดตั้งบน Cloud จำนวน 6 เครื่อง
  - ปรับแต่งระบบลีนุกซ์เพื่อเตรียมติดตั้ง Hadoop
  - ติดตั้งฐานข้อมูล MySQL และ MySQL JDBC
  - ติดตั้ง Parallel command เพื่อสั่งคำสั่งพร้อมกันที่เดียวหลายเครื่อง
  - ติดตั้ง Services ต่าง ๆ ผ่าน Cloudera Manager หรือ Apache Ambari

### วันที่ 2

- การทำ High Availability (HA)
  - การทำ High Availability สำหรับ HDFS
  - การทำ High Availability สำหรับYARN
- การใช้งาน HDFS
  - การใช้คำสั่งhadoop การจัดการไฟล์ในระบบ HDFS
- การใช้งาน MapReduce2 (Yarn)
  - การรันโปรแกรมคำนวณค่า Pi ผ่าน apReduce2
  - การคอมไพล์และรันโปรแกรม MapReduce
  - ตัวอย่างโปรแกรม WordCount
  - การ Monitor MapReduce Task
- การใช้งาน Pig
  - การเขียน Pig Script และรัน
- รู้จักกับ Hive เครื่องมือที่จะช่วยให้เราสามารถสั่ง SQL เพื่อทำ MapReduce ได้
  - การใช้งาน Hive ผ่านคำสั่ง SQL
  - การใช้งาน Hive ผ่านคำสั่ง hive และ beeline
  - เทคนิคการนำเข้าข้อมูล Hive
  - การคิวรี่ข้อมูลที่จัดเก็บบน JSON File
  - รู้จักกับรูปแบบการจัดเก็บข้อมูลอื่น ๆ บน Hive
  - กรณีศึกษาตัวอย่างการใช้งานจริง
- รู้จักกับ Sqoop เครื่องมือที่ใช้เชื่อมต่อกับ JDBC เพื่อนำเข้าข้อมูลจากฐานข้อมูล
  - การติดตั้งและใช้งาน Sqoop
  - การนำเข้าข้อมูลจาก MySQL สู่ HDFS และ Hive
  - การนำออกข้อมูลจาก HDFS และ Hive สู่ MySQL
- รู้จักกับ Flume
  - การใช้งาน Flume กับ log data

### วันที่ 3

- ติดตั้งและใช้งาน JDBC, ODBC สำหรับ Hive และ Impala
- รู้จักกับ Spark
  - ทดสอบการใช้งาน Spark ด้วยโปรแกรมหาค่าPi
  - การใช้งาน Spark ผ่านภาษา python (pyspark)
  - ตัวอย่างการใช้งาน Spark ML ด้วยการรัน K-mean กับชุดข้อมูล Iris
- รู้จักกับ HBase
  - การใช้งานคำสั่งพื้นฐาน HBase Shell
- รู้จักกับ Kafka และใช้งาน
- การใช้งาน WebHDFS API
- การปรับแต่งประสิทธิภาพที่สำคัญสำหรับการใช้งานจริง
- การออกแบบระบบที่เหมาะสม และกรณีศึกษา

## เนื้อหาในส่วนที่แตกต่างกันระหว่าง Cloudera และ Hortonwork

<table class="table table-bordered">
  <thead>
    <tr>
      <th>Cloudera Distribution Hadoop (CDH)</th>
      <th>Hortonwork Data Platform (HDP)</th>
    </tr>
  </thead>
  <tbody>
  <div>
    <tr>
      <td>
         <li>รู้จักกับ Hue Web Interface1</li>
         <li>Enable service Hue</li>
         <li>การใช้งาน Hue UI</li>
         <li>การใช้งาน Hive บน Hue</li>
         <li>การใช้งาน Impala บน Hue</li> 
         <li>การใช้งาน Sqoop บน Hue</li>
         <li>การใช้งาน OOZIE การทำ Workflow</li>    
         <li>โดยการตั้งเวลารันสคริปต์</li>
         <li>รู้จักกับ Impala และใช้งาน</li>
      </td>
      <td>
         <li>รู้จักกับ Apache Zeppelin</li>
         <li>การติดตั้งและใช้งาน Apache Zeppelin UI</li>
         <li>การใช้งาน Hive บน Zeppelin</li>
         <li>การใช้งาน Spark บน Zeppelin</li>
      </td>
    </tr>
  </tbody>
</table>
